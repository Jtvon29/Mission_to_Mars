{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from splinter import Browser\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "from datetime import datetime\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize browser\n",
    "def init_Browser():\n",
    "    # @NOTE: Replace the path with your actual path to the chromedriver\n",
    "    executable_path = {\"executable_path\": \"Resources/chromedriver.exe\"}\n",
    "    return Browser(\"chrome\", **executable_path, headless=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Scrape the Nasa website for the most recent article\n",
    "# Initialize the browser\n",
    "browser = init_Browser()\n",
    "# Visit the Nasa website\n",
    "nasa_url = \"https://mars.nasa.gov/news/?page=0&per_page=40&order=publish_date+desc%2Ccreated_at+desc&search=&category=19%2C165%2C184%2C204&blank_scope=Latest\"\n",
    "#visit the nasa mars site\n",
    "browser.visit(nasa_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scrape the page into soup\n",
    "nasa_html = browser.html\n",
    "nasa_soup = BeautifulSoup(nasa_html, 'html.parser')\n",
    "# Find the most recent article title and paragraph text\n",
    "Article = nasa_soup.select_one(\"ul.item_list li.slide\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'For InSight, Dust Cleanings Will Yield New Science'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Retrieve the article title\n",
    "Title = Article.find(\"div\", class_='content_title').get_text()\n",
    "Title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Wind can be crucial to clearing dust from spacecraft solar panels on Mars. With InSight\\'s meteorological sensors, scientists get their first measurements of wind and dust interacting \"live\" on the Martian surface.  '"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Retrieve the article paragraph\n",
    "Paragraph = Article.find(\"div\", class_='article_teaser_body').get_text()\n",
    "Paragraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Scrape mars image\n",
    "#Visit the website\n",
    "url = 'https://www.jpl.nasa.gov/spaceimages/?search=&category=Mars'\n",
    "browser.visit(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Navigate to the Full size image\n",
    "full_image = browser.find_by_id('full_image')\n",
    "full_image.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find the more info button and click that\n",
    "browser.is_element_present_by_text('more info', wait_time=1)\n",
    "More_info = browser.find_link_by_partial_text('more info')\n",
    "More_info.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Scrape the page into soup\n",
    "html = browser.html\n",
    "image_soup = BeautifulSoup(html, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/spaceimages/images/largesize/PIA18297_hires.jpg'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Find the relative image url\n",
    "Image_url = image_soup.select_one('figure.lede a img').get(\"src\")\n",
    "Image_url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://www.jpl.nasa.gov/spaceimages/images/largesize/PIA18297_hires.jpg'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Use the base url to create an absolute url\n",
    "Featured_image_url = f'https://www.jpl.nasa.gov{Image_url}'\n",
    "Featured_image_url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Scrape the Nasa Twitter page for the most recent weather report\n",
    "#visit the website\n",
    "twitter_url = 'https://twitter.com/marswxreport?lang=en'\n",
    "browser.visit(twitter_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Scrape the page into soup\n",
    "twitter_html = browser.html\n",
    "twitter_soup = BeautifulSoup(twitter_html, 'html.parser')\n",
    "# Find the most recent tweet about the weather\n",
    "Tweet = twitter_soup.find('div', attrs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'find'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-15-dda5474dd30c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# Retieve the Tweet from the feed\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mTweet_weather\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTweet\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'p'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mclass_\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'TweetTextSize TweetTextSize--normal js-tweet-text tweet-text'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mTweet_weather\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'find'"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def scrape_hemi():\n",
    "    \n",
    "    # Scrape the hemisphere images of mars\n",
    "    #Visit the website\n",
    "    hemi_main_url = 'https://astrogeology.usgs.gov/search/results?q=hemisphere+enhanced&k1=target&v1=Mars'\n",
    "    browser.visit(hemi_main_url)\n",
    "    #Scrape the page into soup\n",
    "    hemi_main_html = browser.html\n",
    "    hemi_main_soup = BeautifulSoup(hemi_main_html, 'html.parser')\n",
    "    #Find the path to the full size images\n",
    "    first = hemi_main_soup.find('div', class_='collapsible results')\n",
    "    second = first.find_all('div', class_='description')\n",
    "    #Create an empty list to hold the images\n",
    "    Hemisphere_images = []\n",
    "    #create a loop to find the images of the hemispheres in each link\n",
    "    for x in second:\n",
    "        #go to the link\n",
    "        title = x.a.h3.text\n",
    "        link = 'https://astrogeology.usgs.gov' + x.a['href']\n",
    "        browser.visit(link)\n",
    "        #Scrape the page into soup\n",
    "        hemi_html = browser.html\n",
    "        hemi_soup = BeautifulSoup(hemi_html, 'html.parser')\n",
    "        #find the path of the fullsize image\n",
    "        image_link = hemi_soup.find('div', class_='downloads').find('li').a['href']\n",
    "        #Create dictioanry to hold the urls\n",
    "        hemi_dict = {}\n",
    "        hemi_dict['title'] = title\n",
    "        hemi_dict['link'] = link\n",
    "        Hemisphere_images.append(hemi_dict)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
